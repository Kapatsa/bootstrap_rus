\section{Наименьшая медиана квадратов}

Наименьшая медиана квадратов регрессии, сокращенно $\text{LMS}$, является менее чувствительным методом подбора, чем метод наименьших квадратов. Единственное различие между методом наименьших квадратов и $\text{LMS}$ --- это выбор критерия соответствия. Чтобы обосновать критерий, давайте разделим остаточную квадратичную ошибку (9.7) на размер выборки, получив среднеквадратичные остатки
\begin{equation}
	\dfrac{1}{n} \sum\limits_{i=1}^{n} (y_i-\textbf{c}_i \textbf{b})^2.
\end{equation}
Минимизация (9.38), очевидно, то же самое, что минимизация (9.7). Средние выборок чувствительны к влияющим значениям, а медианы --- нет. Следовательно, чтобы сделать (9.38) менее чувствительным, мы можем заменить среднее значение на медиану, получив \textit{медиану квадратичных остатков}
\begin{equation}
	\text{MSR}(\textbf{b}) = \text{med}(y_i - \textbf{c}_i \textbf{b})^2.
\end{equation}
Оценка $\text{LMS}$ $\bm{\beta}$ --- это значение $\hat{\bm{\beta}}$, минимизирующее $\text{MSR(\textbf{b})}$,
\begin{equation}
	\text{MSR}(\hat{\bm{\beta}}) = \min_{\textbf{b}} [\text{MSR}(\textbf{b})].
\end{equation}

Обратите внимание, что разница между методом наименьших квадратов и $\text{LMS}$ заключается не в выборе модели, которая остается (9.3), а в том, как мы измеряем расхождения между моделью и наблюдаемыми данными. $\text{MSR}(\textbf{b})$ менее чувствителен, чем $\text{RSE}(\textbf{b})$, к удаленным точкам данных. Это можно увидеть на рис. 9.3, где, по-видимому, очень мало различий между квадратичной $\text{LMS}$-аппроксимацией с точкой «?» или без нее. На самом деле разницы нет. Расчетные коэффициенты регрессии равны $(\hat{\beta}_1, \hat{\beta}_2) = (-0.81, 0.0088)$ в обоих случаях.

Можно показать, что \textit{порог} (breakdown) оценки $\text{LMS}$ составляет примерно $50\%$. Порог оценщика --- это наименьшая часть данных, которая может иметь сколь угодно большое влияние на его значение. Другими словами, оценщик имеет порог $\alpha$, если по крайней мере $m = \alpha \cdot n$ точек данных будут «плохими», прежде чем он разделит. Высокий порог --- это хорошо, при этом $50\%$ --- это наибольшее значение, которое имеет смысл (если $\alpha> 50\%$, неясно, какие из них являются хорошими, а какие плохими). Например, среднее значение выборки имеет порог $1 / n$, поскольку, изменяя только одно значение данных, мы можем заставить среднее значение выборки принимать любое значение. Медиана выборки имеет порог $50\%$, что отражает тот факт, что она менее чувствительна к отдельным значениям. Оценщик регрессии наименьших квадратов наследует чувствительность среднего и имеет порог $1 / n$, в то время как оценщик наименьших средних квадратов, как и медиана, имеет порог примерно $50\%$.

Насколько точны $\text{LMS}$ оценки $\hat{\beta}_1, \hat{\beta}_2$? Нет четкой формулы, подобной (9.20) для стандартных ошибок $\text{LMS}$. (Нет четкой формулы для самих оценок $\text{LMS}$. Они вычисляются с использованием алгоритма выборки с возвращением.) Стандартные ошибки в таблице 9.5 были получены методами бутстрепа. Стандартные ошибки в строке 3 основаны на парах выборок без возвращения, как в разделе 7.3. Бустреп набор данных имеет форму $\textbf{x}^* = ((\textbf{c}_1^*, y_1^*), (\textbf{c}_2^*, y_2^*), \ldots, (\textbf{c}_n^*, y_n^*))$, как в (9.31), где $\textbf{c}_i = (z_i, z_i^2)$. После генерации $\textbf{x}^*$ была получена бутстреп репликация $\hat{\bm{\beta}}^*$ для вектора регрессии $\text{LMS}$ как минимизатор медианы квадратичных остатков для бустреп данных, то есть минимизатор по $\textbf{b}$ для
\begin{equation}
	\text{med}(y_i^* - \textbf{c}_i^* \textbf{b})^2
\end{equation}
$B = 400$ репликаций бустрепа дают оценочные стандартные ошибки в строке 3 таблицы 9.5. Обратите внимание, что $\hat{\beta}_2$ не намного больше нуля.

Признаками в данных выживаемости клеток были фиксированные числа, установленные исследователем: она выбрала дозы $$1.175,1.175,2.35,\ldots,14.100,$$ чтобы провести хороший эксперимент по различению линейной и квадратичной моделей радиационной выживаемости. Это заставляет нас больше интересоваться бустреп-остатками (9.32), нежели бустреп-парами. Тогда бустреп наборы данных $\textbf{x}^*$ будут иметь те же векторы признаков $\textbf{c}_1, \textbf{c}_2,\ldots,\textbf{c}_n$, которые исследователь намеренно использовал в эксперименте.

Модель (9.4), (9.5) не совсем подходит для данных о выживаемости клеток. Глядя на рисунок 9.3, мы видим, что зависимая переменная $y_i$ более рассеянна при больших значениях $z$. Это похоже на ситуацию с холостирамином на рис. 9.2, за исключением того, что у нас недостаточно точек для построения хороших процентилей регрессии. Грубо говоря, мы будем предполагать, что ошибки линейной модели линейно возрастают с дозой $z$. Это равносильно замене (9.4) на
\begin{equation}
	y_i = \textbf{c}_i \bm{\beta} + z_i \varepsilon_i \text{  для  } i=1,2,\ldots,14.
\end{equation}
Мы по-прежнему предполагаем, что $(\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_n)$ --- случайная выборка из некоторого распределения $F$, (9.5). Для модели квадратичной регрессии $\textbf{c}_i = (z_i, z_i^2)$.

Модель вероятности для (9.42), как и раньше, равна $P = (\bm{\beta}, F)$; $\bm{\beta}$ было оценено при помощи $\text{LMS}$, $\hat{\bm{\beta}} = (-0.83, 0.0114)$. Затем $F$ было оценено с помощью $\hat{F}$, эмпирического распределения величин $(y_i - \textbf{c}_i \hat{\bm{\beta}}/z_i)$, $i=1,2,\ldots,14$.

Строка 4 таблицы 9.5 сообщает о бутстреп стандартных ошибках для оценок медиан квадратичных остатков $\hat{\beta}_1$ и $\hat{\beta}_2$, полученных из $B = 200$ бутстреп репликаций, с бутстреп-остатками в модели (9.42). Стандартные ошибки заметно меньше, чем при бустрепе пар. (Но недостаточно мал, чтобы сделать $\hat{\beta}_2$ значимо отличным от нуля.) Стандартные ошибки в строке 4 следует рассматривать с осторожностью, поскольку данные модели (9.42) лишь делают слабое предположение. Самым важным в представлении модели было проиллюстрировать, как бутстреп остатков может быть выполнен в ситуациях, более сложных, чем (9.4).